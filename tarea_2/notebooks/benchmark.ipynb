{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to consider when developing an app\n",
    "\n",
    "This is an experiment to benchmark 3 algos\n",
    "\n",
    "The problem is about graphs. The goal is to found that every node is connected to at least two edges. Every edge has a cost. The goal is to minimize the cost of the edges.\n",
    "\n",
    "## What did I need for this app?\n",
    "\n",
    "### 1. Benchmark system\n",
    "\n",
    "Params needed:\n",
    "\n",
    "- **Algoritihms**: list of algorithms to test\n",
    "- **Input sizes**: list of **n** number of nodes to test \n",
    "- **Repetitions**: number of repetitions to test\n",
    "- **Output**: benchmark results\n",
    "  - **cost** = cost of the edges\n",
    "  - **time** = time to run the algorithm\n",
    "  - **space** = space to run the algorithm\n",
    " \n",
    "Outcomes needed:\n",
    "- Cost\n",
    "  - function to calculate the value ´evaluate_solution´\n",
    "- Temporal Complexity\n",
    "  - Package: ´´timeit´´\n",
    "- Space Complexity\n",
    "  - Package: ´´tracemalloc´´ or ´´memory_profiler´´\n",
    "\n",
    "\n",
    "### 2. Algorithms\n",
    "\n",
    "**Preconditions**\n",
    "\n",
    "Prepare a function to generate a graph.\n",
    "- Params:\n",
    "  - **n**: number of nodes\n",
    "  - **min_weight**: minimum weight of the edges\n",
    "  - **max_weight**: maximum weight of the edges\n",
    "\n",
    "With ´networkx´ library, we can generate full connected graphs to then apply the algorithms.\n",
    "\n",
    "**Implement the following algorithms**\n",
    "\n",
    "- Brute force\n",
    "- Heuristic based in Kruskal algorithm\n",
    "- Metaheuristic based in simulated annealing\n",
    "\n",
    "**How to manage a graph to do algorithm operations?**\n",
    "\n",
    "Use ´´networkx´´ library for this. This way I have the data structure to manage the graph and the algorithms to work with it.\n",
    "\n",
    "Moreover, this library have **minimum spanning tree algorithm**, which is useful for the heuristic algorithm and the simulated annealing algorithm. This algorithm used Kruskal algorithm to find the minimum spanning tree.\n",
    "\n",
    "\n",
    "### 3. Results Analysis\n",
    "\n",
    "We need to compare the results of the algorithms.\n",
    "\n",
    "We can use plots, dicts or df to show the results.\n",
    "\n",
    "Libraries:\n",
    "- ´´matplotlib´´\n",
    "- ´´pandas´´\n",
    "- ´´numpy´´\n",
    "- ´´seaborn´´\n",
    "\n",
    "The results to compare for each number of node input are:\n",
    "\n",
    "- Solution cost \n",
    "- Solution reliability???\n",
    "- Time Complexity\n",
    "- Space Complexity\n",
    "\n",
    "Because each algorithm run with 3 different input sized and each of that runs are repeated, we can calculate:\n",
    "\n",
    "- Average\n",
    "- Min\n",
    "- Max\n",
    "\n",
    "\n",
    "Of the results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototyping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries used\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import timeit\n",
    "import tracemalloc\n",
    "import matplotlib.pyplot as plt\n",
    "#scipy needed for nerworkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## networkx example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_random_graph(num_nodes, num_edges):\n",
    "    return nx.gnm_random_graph(num_nodes, num_edges)\n",
    "\n",
    "# Example usage:\n",
    "test_graph = generate_random_graph(4, 2)\n",
    "\n",
    "# print the graph\n",
    "print(test_graph.edges())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random graph generation with networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specific problem graph generator\n",
    "def generate_random_graph(num_nodes: int, min_cost: int, max_cost: int):\n",
    "    \"\"\"\n",
    "    Undirected graph generator, where the graph is fully connected\n",
    "    and the weight of the edges are random cost and a reliability value.\n",
    "    Parameters:\n",
    "    ----------\n",
    "    num_nodes: int\n",
    "        Number of nodes in the graph\n",
    "    min_cost: int\n",
    "        Minimum value of the cost of the edges\n",
    "    max_cost: int\n",
    "        Maximum value of the cost of the edges\n",
    "    \"\"\"\n",
    "    # num_edges to have a full connected graph\n",
    "    num_edges = num_nodes * (num_nodes - 1) // 2\n",
    "    # values of weights are random integers between 1 and 100\n",
    "    graph = nx.gnm_random_graph(num_nodes, num_edges)\n",
    "    for u, v in graph.edges:\n",
    "        # cost and reliability of each edge connection beetween nodes\n",
    "        graph[u][v]['values'] = {\n",
    "            \"cost\": np.random.randint(min_cost, max_cost),\n",
    "            \"reliability\": np.random.uniform(0, 1),\n",
    "        }\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the function\n",
    "base_graph = generate_random_graph(num_nodes=5, min_cost=1, max_cost=20)\n",
    "\n",
    "# print matrix with value tuples\n",
    "print(\"Graph Nodes:\")\n",
    "print(base_graph.nodes)\n",
    "\n",
    "\n",
    "print(\"\\nGraph Edges with Attributes:\")\n",
    "for u, v, attr in base_graph.edges(data=True):\n",
    "    print(f\"({u}, {v}) -> cost: {attr['values']['cost']}, reliability: {attr['values']['reliability']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_solution(graph, edges):\n",
    "    # dummy solution with the cost of all edges\n",
    "    return sum(graph[u][v]['values']['cost'] for u, v in edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark system\n",
    "\n",
    "### Sample size calculation\n",
    "To compute the ideal sample por each algorithm with a given input size, we need to calculate the number of repetitions to get a good sample.\n",
    "\n",
    "#### Option size 1 - 90% confidence and 10% error\n",
    "With 90% confidence and 10% error, we can calculate the sample size with the following formula:\n",
    "- z = 1.645: z-score for 90% confidence\n",
    "- e = 0.1  : error\n",
    "- p = 0.5  : population proportion\n",
    "- n        : sample size\n",
    "\n",
    "n = (z^2 * p * (1-p)) / e^2\n",
    "\n",
    "n = 67,6\n",
    "\n",
    "#### Option size 2 - 80% confidence and 20% error\n",
    "With 80% confidence and 20% error, we can calculate the sample size with the following formula:\n",
    "- z = 1.282: z-score for 80% confidence\n",
    "- e = 0.2  : error\n",
    "- p = 0.5  : population proportion\n",
    "- n        : sample size\n",
    "\n",
    "n = (z^2 * p * (1-p)) / e^2\n",
    "\n",
    "n = 10,3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_solution(graph, edges):\n",
    "    # dummy solution with the cost of all edges\n",
    "    return sum(graph[u][v]['values']['cost'] for u, v in edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy algorithms\n",
    "import time\n",
    "\n",
    "def brute_force(graph):\n",
    "    # Dummy implementation\n",
    "    list_edges = list(graph.edges(data=True))\n",
    "    return {\"total_cost\": 2, \"reliability\": 0.3}\n",
    "\n",
    "def kruskal_heuristic(graph):\n",
    "    # Dummy implementation\n",
    "    list_edges = list(graph.edges(data=True))\n",
    "    return {\"total_cost\": 2, \"reliability\": 0.3}\n",
    "\n",
    "def simulated_annealing(graph: nx.Graph, max_iter=100, minimal_temp=20.1, cooling_rate=0.99):\n",
    "    # Dummy implementation\n",
    "    list_edges = list(graph.edges(data=True))\n",
    "    return {\"total_cost\": 2, \"reliability\": 0.3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other considerations**\n",
    "\n",
    "In the next function, we are going to do the benchmark for each algorithm with the sample size calculated.\n",
    "\n",
    "Output:\n",
    "- time_response_variable    : it will be a resulto with the average, min and max time for each algorithm\n",
    "- space_results   : it will be a resulto with the average, min and max space for each algorithm\n",
    "- utility_response_variable : it will be the solution of the algorithm.\n",
    "\n",
    "Having mean, min and max values, we can compare the versatility of the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark wrapper function\n",
    "def benchmark_algorithm(algorithm, graph: nx.graph, number_of_runs=67):\n",
    "    \"\"\"\n",
    "    Compute the benchmark results for a given algorithm with a graph\n",
    "    and a specified number of runs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    algorithm : function\n",
    "        The algorithm to benchmark.\n",
    "    graph : nx.Graph\n",
    "        The graph on which the algorithm will be run.\n",
    "    number_of_runs : int, optional\n",
    "        Number of runs to perform for each benchmark. Default is 67.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    time_response_variable : dict\n",
    "        Dict with mean, min and max of execution times.\n",
    "    memory_response_variable : dict\n",
    "        Dict with mean, min and max of peak memory usages\n",
    "    utility_response_variable : list of Solutions\n",
    "        List of utility values total cost and fiability for each run.\n",
    "    \"\"\"\n",
    "\n",
    "    # Variables to store the output\n",
    "    ## The term \"response variable\" is used in Experimental Design to refer to the output of the experiment\n",
    "    time_response_variable = {\"mean\": 0, \"min\": 0, \"max\": 0}\n",
    "    memory_response_variable = {\"mean\": 0, \"min\": 0, \"max\": 0}\n",
    "    # set of utility values\n",
    "    utility_response_variable = []\n",
    "\n",
    "    # Variables to store the benchmark results\n",
    "    time_bench_results = []\n",
    "    memory_bech_results = []\n",
    "\n",
    "    for _ in range(number_of_runs):\n",
    "        # Track time using timeit\n",
    "        timer = timeit.Timer(lambda: algorithm(graph))\n",
    "        exec_time = timer.timeit(number=1)\n",
    "\n",
    "        # Track memory using tracemalloc\n",
    "        tracemalloc.start()\n",
    "        result = algorithm(graph)\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "\n",
    "        # Collect results\n",
    "        time_bench_results.append(exec_time)\n",
    "        memory_bech_results.append(peak / 10**6)  # Convert to MB\n",
    "        utility_response_variable.append(result['total_cost'])\n",
    "\n",
    "    # Compute the mean, min and max of the results\n",
    "    time_response_variable[\"mean\"] = np.mean(time_bench_results)\n",
    "    time_response_variable[\"min\"] = np.min(time_bench_results)\n",
    "    time_response_variable[\"max\"] = np.max(time_bench_results)\n",
    "\n",
    "    memory_response_variable[\"mean\"] = np.mean(memory_bech_results)\n",
    "    memory_response_variable[\"min\"] = np.min(memory_bech_results)\n",
    "    memory_response_variable[\"max\"] = np.max(memory_bech_results)\n",
    "\n",
    "    # Clean the repited utility values\n",
    "    utility_response_variable = list(set(utility_response_variable))\n",
    "\n",
    "    return time_response_variable, memory_response_variable, utility_response_variable\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the bench after setup**\n",
    "\n",
    "We need to run the benchmark after the setup of the algorithms. This way we can compare the results of the algorithms.\n",
    "\n",
    "**Objective**:\n",
    "\n",
    "- Display the results of the benchmark setup.\n",
    "  - As a table or plots\n",
    "\n",
    "To accomplish this, we need to have a data object to save the results of \n",
    "benchmark_algorithm function. And we need to save that with each number of nodes.\n",
    "\n",
    "For this we can use a **dataframe**:\n",
    "- Columns:\n",
    "  - algorithm_name\n",
    "  - input_size\n",
    "  - time_mean\n",
    "  - time_min\n",
    "  - time_max\n",
    "  - space_mean\n",
    "  - space_min\n",
    "  - space_max\n",
    "  - total_cost\n",
    "\n",
    "Then, we can use this dataframe to plot the results or show tables.\n",
    "\n",
    "Output:\n",
    "- None, only display the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot_results(df: pd.DataFrame, column: str):\n",
    "    \"\"\"\n",
    "    Plot the time results of the benchmark\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Dataframe with the results of the benchmark\n",
    "    title : str\n",
    "        Title of the plot\n",
    "    \"\"\"\n",
    "    ## prepere the data\n",
    "    algorithms = df[\"algorithm_name\"].unique()\n",
    "    input_sizes = df[\"input_size\"].unique()\n",
    "    x = np.arange(len(input_sizes))  # x-axis positions for groups\n",
    "    width = 0.2  # width of the bars\n",
    "\n",
    "    ## create grouped data\n",
    "    figure, axes = plt.subplots(figsize=(10, 6))\n",
    "    for i, algorithm in enumerate(algorithms):\n",
    "        algorithm_data = df[df[\"algorithm_name\"] == algorithm]\n",
    "        axes.bar(x + i * width, algorithm_data[column], width, label=algorithm)\n",
    "\n",
    "    ## Customize plot\n",
    "    axes.set_xlabel(\"Input Size\", fontsize=12)\n",
    "    axes.set_ylabel(column, fontsize=12)\n",
    "    axes.set_title(\"Grouped Bar Chart: Algorithm Comparison\", fontsize=14)\n",
    "    axes.set_xticks(x + width)\n",
    "    axes.set_xticklabels(input_sizes)\n",
    "    axes.legend(title=\"Algorithms\")\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_plot_results(df: pd.DataFrame, column: str):\n",
    "    \"\"\"\n",
    "    Plot the time results of the benchmark\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Dataframe with the results of the benchmark\n",
    "    title : str\n",
    "        Title of the plot\n",
    "    \"\"\"\n",
    "    # Prepare the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    ## prepere the data\n",
    "\n",
    "    algorithms = df[\"algorithm_name\"].unique()\n",
    "    input_sizes = df[\"input_size\"].unique()\n",
    "    width = 0.2  # width of the bars\n",
    "\n",
    "    # Iterate through each algorithm and plot its line\n",
    "    for algorithm in algorithms:\n",
    "        algorithm_data = df[df[\"algorithm_name\"] == algorithm]\n",
    "        plt.plot(\n",
    "            algorithm_data[\"input_size\"],\n",
    "            algorithm_data[column],\n",
    "            marker='o',\n",
    "            label=algorithm\n",
    "        )\n",
    "\n",
    "    ## Customize plot\n",
    "    plt.xlabel(\"Input Size\", fontsize=12)\n",
    "    plt.ylabel(column, fontsize=12)\n",
    "    plt.title(\"Line Chart: Algorithm Comparison\", fontsize=14)\n",
    "    plt.legend(title=\"Algorithms\", fontsize=10)\n",
    "    plt.grid(axis=\"both\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "columns = [\"algorithm_name\", \"input_size\", \"time_mean\", \"time_min\",\"time_max\",\n",
    "           \"memory_mean\", \"memory_min\", \"memory_max\", \"total_cost\"]\n",
    "\n",
    "# Main function to run the benchmark\n",
    "def run_benchmarks():\n",
    "\n",
    "    # Algorithms to test.\n",
    "    algorithms = {\n",
    "        \"Brute Force\": brute_force,\n",
    "        \"Kruskal Heuristic\": kruskal_heuristic,\n",
    "        \"Simulated Annealing\": simulated_annealing,\n",
    "    }\n",
    "\n",
    "    # Input control variables\n",
    "    number_of_nodes = [20, 40, 60]\n",
    "    min_cost = 1\n",
    "    max_cost = 20\n",
    "\n",
    "    # Dataframe to store the results\n",
    "    results_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    #for the same input size we use the same graph to test the algorithms\n",
    "    for size in number_of_nodes:\n",
    "        graph = generate_random_graph(size, min_cost, max_cost)\n",
    "\n",
    "        # Results for each algorithm\n",
    "        results = {}\n",
    "\n",
    "        for name, algorithm in algorithms.items():\n",
    "            time_response_variable, memory_response_variable, utility_response_variable = benchmark_algorithm(\n",
    "                    algorithm, graph, number_of_runs=10)\n",
    "\n",
    "            results = {\n",
    "                \"algorithm_name\": name,\n",
    "                \"input_size\": size,\n",
    "                \"time_mean\": time_response_variable[\"mean\"],\n",
    "                \"time_min\": time_response_variable[\"min\"],\n",
    "                \"time_max\": time_response_variable[\"max\"],\n",
    "                \"memory_mean\": memory_response_variable[\"mean\"],\n",
    "                \"memory_min\": memory_response_variable[\"min\"],\n",
    "                \"memory_max\": memory_response_variable[\"max\"],\n",
    "                \"total_cost\": utility_response_variable,\n",
    "            }\n",
    "\n",
    "            # Convert results to a DataFrame and append to the main DataFrame\n",
    "            new_row = pd.DataFrame([results])\n",
    "            results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "\n",
    "    # Show the results in a table\n",
    "    print(results_df)\n",
    "\n",
    "    # Plot the results mathplotlib\n",
    "    # Time mean, max and min separately\n",
    "    bar_plot_results(results_df, \"time_mean\")\n",
    "    bar_plot_results(results_df, \"time_min\")\n",
    "    bar_plot_results(results_df, \"time_max\")\n",
    "    line_plot_results(results_df, \"time_mean\")\n",
    "    line_plot_results(results_df, \"time_min\")\n",
    "    line_plot_results(results_df, \"time_max\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run the benchmark\n",
    "run_benchmarks()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
